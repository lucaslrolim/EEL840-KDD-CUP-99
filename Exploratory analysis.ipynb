{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing libraries\n",
    "%matplotlib inline\n",
    "import numpy                   as np\n",
    "import pandas                  as pd\n",
    "import matplotlib              as pl\n",
    "import seaborn                 as sns\n",
    "from  sklearn.model_selection  import train_test_split\n",
    "from  sklearn.tree             import DecisionTreeClassifier\n",
    "from sklearn.neural_network    import MLPClassifier\n",
    "from  sklearn.metrics          import accuracy_score\n",
    "from  sklearn                  import tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise Exploratória"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos começar lendo o conjunto de dados e definindo o nome correto para suas colunas. Além disso, vamos criar dois conjuntos de dado ¨de apoio¨, um contendo apenas variáveis discretas/categóricas e outro com as variáveis contínuas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Reading .csv files\n",
    "df = pd.read_csv(\"data/corrected.csv\",sep=',',header=None)\n",
    "\n",
    "## Defining columns\n",
    "col_names  = pd.read_csv(\"data/column_names.csv\",sep=',',header=None)[0].values\n",
    "df.columns = col_names\n",
    "\n",
    "\n",
    "## Identifying columns as categorical or not\n",
    "categorical_columns = [\"protocol_type\",\"service\",\"flag\",\"land\",\"logged_in\",\"root_shell\",\"su_attempted\",\n",
    "                       \"is_host_login\",\"is_guest_login\",\"label\"]\n",
    "df_categorical      = df[categorical_columns]\n",
    "df_non_categorical  = df.drop(categorical_columns, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## A glance at the non categorical data\n",
    "df_non_categorical.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise de variáveis discretas\n",
    "\n",
    "\n",
    "### Relação entre Ataques e Serviços de Rede\n",
    "\n",
    "Vamos começar analisando a relação entre o serviço de rede utilizado pelo destino dos pacotes e os ataques identificados. Para tal, vamos plotar um gráfico de barras que mostra o percentual de participação de cada serviço de rede em cada ataque.\n",
    "\n",
    "**De modo a facilitar a visualização, vamos considerar apenas variáveis que representem ao menos 1% do percentual total de ataques ou tipos de serviço de rede.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Create a subset of main dataset (f_data) that contains only services and labels that represents at\n",
    "## least 1% of the total cases\n",
    "\n",
    "f_services = pd.crosstab(index=df[\"service\"],columns=\"count\")\n",
    "f_services = f_services/len(df)\n",
    "f_services = f_services[f_services[\"count\"] > 0.01]\n",
    "\n",
    "f_attacks = pd.crosstab(index=df[\"label\"],columns=\"count\")\n",
    "f_attacks = f_attacks/len(df)\n",
    "f_attacks = f_attacks[f_attacks[\"count\"] > 0.01]\n",
    "\n",
    "f_data = df[df['service'].isin(list(f_services.index))]\n",
    "f_data = f_data[f_data['label'].isin(list(f_attacks.index))]\n",
    "\n",
    "## Create the plot\n",
    "\n",
    "attack_data = pd.crosstab(index = f_data[\"label\"], columns=f_data[\"service\"])\n",
    "frequency_table_attack = (attack_data/attack_data.sum())\n",
    "\n",
    "frequency_table_attack.plot(kind=\"bar\", \n",
    "                 figsize=(8,8),\n",
    "                 stacked=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relação entre Ataques e Tipos de Protocolo\n",
    "\n",
    "Utilizando a mesma abordagem anterior, iremos avaliar o percentual de participcação de cada protocolo de rede em cada tipo de ataque.\n",
    "\n",
    "O objetivo desse análise é visualziar se existe alguma característica que foge do conhecido pela literatura."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "## Create a subset of main dataset (f_data) that contains only protocols and labels that represents at\n",
    "## least 1% of the total cases\n",
    "\n",
    "f_protocol = pd.crosstab(index=df[\"protocol_type\"],columns=\"count\")\n",
    "f_protocol = f_protocol/len(df)\n",
    "f_protocol = f_protocol[f_protocol[\"count\"] > 0.01]\n",
    "\n",
    "f_data = df[df['protocol_type'].isin(list(f_protocol.index))]\n",
    "f_data = f_data[f_data['label'].isin(list(f_attacks.index))]\n",
    "\n",
    "## Create a Two-Way Table\n",
    "\n",
    "relationship_protocoal_attack = pd.crosstab(index=f_data[\"label\"], \n",
    "                          columns=f_data[\"protocol_type\"])\n",
    "## Plot the Two-Way Table\n",
    "relationship_protocoal_attack.plot(kind=\"bar\", \n",
    "                 figsize=(8,8),\n",
    "                 stacked=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Participação percentual de cada tipo de protocolo de rede\n",
    "\n",
    "O gráfico abaixo visa dar uma visão geral da participação de cada tipo de protocolo na rede na qual os dados foram coletados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create a cross tab dataframe\n",
    "protocol_data = pd.crosstab(index = df[\"protocol_type\"],columns=\"Protocol type\")\n",
    "frequency_table_protocol = (protocol_data/protocol_data.sum())\n",
    "\n",
    "## Plot the dataframe\n",
    "frequency_table_protocol.plot.bar();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Análise de variáveis contínuas\n",
    "\n",
    "### Histogramas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_non_categorical.hist(figsize = (34,40));"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise dos usuários logados no sistema\n",
    "\n",
    "Julgamos interessante avalair o padrão de comportamento dos usuários que de alguma forma estão logados na rede. Dessa forma, iremos criar gráficos que analisam se alguma ameaça foi detectada para os seguintes tipos de usuários: usuários simplesmente logados, usuários logados como host, usuários logados como convidados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df_temp = df[[\"is_guest_login\",\"label\"]]\n",
    "df_temp = df_temp.loc[df_temp['is_guest_login'] == 1]\n",
    "df_temp.groupby([\"label\"]).count().plot(kind='bar', title =\"Número de registros\", figsize=(10, 5), legend=True, fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Análise de dados redundantes\n",
    "\n",
    "De acordo com Tavallaee (2009), o grande número de registros redundantes pode enviesar os classificadores para os registros mais frequentes. Desssa forma, o autor propõe um novo dataset, removendo as duplicatas. Dessa forma, analisaremos o dataset KDD99 completo e o sem as duplicatas. Para tal analisaremos a razão entre os dados únicos e os duplicados e plotaremos os gráficos de relação entre os ataques e serviços, e entre os ataques e protocolos utilizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_temp = df[[\"is_host_login\",\"label\"]]\n",
    "df_temp = df_temp.loc[df_temp[\"is_host_login\"] == 1]\n",
    "df_temp.groupby([\"label\"]).count().plot(kind='bar', title =\"Número de registros\", figsize=(10, 5), legend=True, fontsize=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Análise de dados redundantes\n",
    "\n",
    "De acordo com Tavallaee (2009), o grande número de registros redundantes pode enviesar os classificadores para os registros mais frequentes. Desssa forma, o autor propõe um novo dataset, removendo as duplicatas. Dessa forma, analisaremos o dataset KDD99 completo e o sem as duplicatas. Para tal analisaremos a razão entre os dados únicos e os duplicados e plotaremos os gráficos de relação entre os ataques e serviços, e entre os ataques e protocolos utilizados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df.drop_duplicates()\n",
    "percentage = 1 - float(df2.shape[0])/float(df.shape[0])\n",
    "print(\"O dataset sem duplicatas é {}% menor que o dataset original\".format(percentage * 100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelos de classificação"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de executar nossos modelos, vamos realizar algumas modificações em nosso conjunto de dados. São elas:\n",
    "\n",
    "- Transformar categorias que são strings em categorias numéricas \n",
    "- Atribuir uma categoria binária de ataque ou não ataque para cada um dos resgitros\n",
    "- Criar um conjunto de dados para treino e outro para teste e validação dos modelos, usando a proporção (0.8/02)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mapping strings categories to int\n",
    "\n",
    "df_binary = df.copy()\n",
    "df_binary.loc[df_binary.label != 'normal.','label']= 1\n",
    "df_binary.loc[df_binary.label == 'normal.','label']= 0\n",
    "\n",
    "c1 = df_binary.protocol_type.unique()\n",
    "c2 = df_binary.service.unique()\n",
    "c3 = df_binary.flag.unique()\n",
    "\n",
    "d_protocol = {}\n",
    "d_service = {}\n",
    "d_wtver = {}\n",
    "\n",
    "for i in range(len(c1)):\n",
    "    d_protocol[c1[i]] = i\n",
    "\n",
    "for i in range(len(c2)):\n",
    "    d_service[c2[i]] = i\n",
    "    \n",
    "for i in range(len(c3)):\n",
    "    d_wtver[c3[i]] = i\n",
    "\n",
    "for i in d_protocol:\n",
    "    df_binary = df_binary.replace(i, d_protocol[i])\n",
    "for i in d_service:\n",
    "    df_binary = df_binary.replace(i, d_service[i])\n",
    "for i in d_wtver:\n",
    "    df_binary = df_binary.replace(i, d_wtver[i])\n",
    "    \n",
    "\n",
    "import random\n",
    "random.seed(1610)\n",
    "\n",
    "## Split the data in train and test datasets\n",
    "\n",
    "train = df_binary.sample(frac = 0.8,random_state=200)\n",
    "test  = df_binary.drop(train.index)\n",
    "\n",
    "X = train.copy()\n",
    "Y = pd.factorize(X['label'])[0]\n",
    "X = X.drop('label', 1)\n",
    "\n",
    "\n",
    "testInput =  test.copy()\n",
    "correct = pd.factorize(testInput['label'])[0]\n",
    "testInput = testInput.drop('label', 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Árvore de regressão"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = clf.fit(df.iloc[:,:41], df[41])\n",
    "p = clf.predict(df.iloc[:,:41])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "randomForest = RandomForestClassifier()\n",
    "randomForest.fit(X,Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "featuresImportance = {}\n",
    "for i in range(len(df.columns[:len(df.columns)-1])):\n",
    "    featuresImportance[df.columns[:len(df.columns)-1][i]] = randomForest.feature_importances_[i]\n",
    "\n",
    "s = [(k, featuresImportance[k]) for k in sorted(featuresImportance, key=featuresImportance.get, reverse=True)]\n",
    "\n",
    "print(\"As 5 features mais relevantes para o modelo foram: \\n\")\n",
    "for i in range (5):\n",
    "    print(\"%s, com influência: %s\" % (s[i][0],s[i][1]))\n",
    "\n",
    "\n",
    "randomForest.score(testInput,correct)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn import svm\n",
    "SVM = svm.SVC()\n",
    "SVM.fit(X, Y)\n",
    "\n",
    "print(\"A acurácia do modelo foi de: \" % (SVM.score(testInput,correct))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Naives Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "gnb = GaussianNB()\n",
    "gnb.fit(X,Y)\n",
    "print(\"A acurácia do modelo foi de: \" % (gnb.score(testInput,correct))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Removing dupicates\n",
    "df_no_duplicates = df.drop_duplicates()\n",
    "percentage = 1 - float(df_no_duplicates[\"protocol_type\"].count())/float(df[\"protocol_type\"].count())\n",
    "print(\"O dataset sem duplicatas é {}% menor que o dataset original\".format(percentage * 100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract all strings from columns\n",
    "protocols = df[\"protocol_type\"].unique()\n",
    "services  = df[\"service\"].unique()\n",
    "flags     = df[\"flag\"].unique()\n",
    "\n",
    "# Mapping strings to int\n",
    "d_protocol = {}\n",
    "d_service  = {}\n",
    "d_flags    = {}\n",
    "for i in range(len(protocols)):\n",
    "    d_protocol[protocols[i]] = i\n",
    "\n",
    "for i in range(len(services)):\n",
    "    d_service[services[i]] = i\n",
    "    \n",
    "for i in range(len(flags)):\n",
    "    d_flags[flags[i]] = i\n",
    "\n",
    "# Replace strings in dataframe\n",
    "for i in d_protocol:\n",
    "    df = df.replace(i, d_protocol[i])\n",
    "print(\"Replaced protocols\")\n",
    "for i in d_service:\n",
    "    df = df.replace(i, d_service[i])\n",
    "print(\"Replaced services\")\n",
    "for i in d_flags:\n",
    "    df = df.replace(i, d_flags[i])\n",
    "print(\"Replaced flags\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = DecisionTreeClassifier()\n",
    "#clf = clf.fit(df.iloc[:,:41], df[\"label\"])\n",
    "#p   = clf.predict(df.iloc[:,:41])\n",
    "cross_val_score(clf, df.iloc[:,:41], df[\"label\"], cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "success = 0\n",
    "fail    = 0\n",
    "for i in range(len(p)):\n",
    "    if (df[\"label\"][i] == p[i]):\n",
    "        success += 1\n",
    "    else:\n",
    "        fail += 1\n",
    "print (\"Sucess: {}\".format(success))\n",
    "print (\"Fail: {}\".format(fail))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"label\"][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.values[:, :41]\n",
    "Y = df.values[:,41]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split( X, Y, test_size = 0.3, random_state = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_gini = DecisionTreeClassifier(criterion = \"gini\", random_state = 100,\n",
    "                               max_depth=3, min_samples_leaf=5)\n",
    "clf_gini.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf_entropy = DecisionTreeClassifier(criterion = \"entropy\", random_state = 100,\n",
    " max_depth=3, min_samples_leaf=5)\n",
    "clf_entropy.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_gini = clf_gini.predict(X_test)\n",
    "y_pred_gini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_en = clf_entropy.predict(X_test)\n",
    "y_pred_en"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print (\"Accuracy using Gini Index is {}%\".format(accuracy_score(y_test,y_pred_gini)*100))\n",
    "print (\"Accuracy using Entropy is {}%\".format(accuracy_score(y_test,y_pred_en)*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
